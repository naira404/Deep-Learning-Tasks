# -*- coding: utf-8 -*-
"""CXR_CT_COUGH_Task2_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AK6ZGhdFPDlPH8naeO0TLQUMvN_652an
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("hossamfakher/cxr-ct-cough")

print("Path to dataset files:", path)

import os
import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
from sklearn.metrics import classification_report

# Use the path where the dataset was downloaded by kagglehub
data_path = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3"
inner_path = os.path.join(data_path, "processed_dataset")
print(os.listdir(inner_path))

data_path_im = os.path.join(inner_path, "processed_dataset")
print(os.listdir(data_path_im))

# CXR
input_cxr = Input(shape=(224, 224, 3), name='CXR_Input')
x1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_cxr)
x1 = MaxPooling2D((2,2))(x1)
x1 = Conv2D(64, (3,3), activation='relu', padding='same')(x1)
x1 = MaxPooling2D((2,2))(x1)
x1 = Flatten()(x1)
x1 = Dense(128, activation='relu')(x1)

# CT
input_ct = Input(shape=(224, 224, 3), name='CT_Input')
x2 = Conv2D(32, (3,3), activation='relu', padding='same')(input_ct)
x2 = MaxPooling2D((2,2))(x2)
x2 = Conv2D(64, (3,3), activation='relu', padding='same')(x2)
x2 = MaxPooling2D((2,2))(x2)
x2 = Flatten()(x2)
x2 = Dense(128, activation='relu')(x2)

# Cough sound
input_audio = Input(shape=(128, 128, 1), name='Audio_Input')
x3 = Conv2D(32, (3,3), activation='relu', padding='same')(input_audio)
x3 = MaxPooling2D((2,2))(x3)
x3 = Conv2D(64, (3,3), activation='relu', padding='same')(x3)
x3 = MaxPooling2D((2,2))(x3)
x3 = Flatten()(x3)
x3 = Dense(128, activation='relu')(x3)

# Concat
merged = Concatenate()([x1, x2, x3])
z = Dense(256, activation='relu')(merged)
z = Dropout(0.4)(z)
output = Dense(4, activation='softmax', name='Output')(z)

fusion_model = Model(inputs=[input_cxr, input_ct, input_audio], outputs=output)

fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
fusion_model.summary()

data_path = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3/processed_dataset/processed_dataset"

print("Folders inside dataset:", os.listdir(data_path))

# Create training generators
img_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# üîπ CXR generator
cxr_train = img_gen.flow_from_directory(
    os.path.join(data_path, "CXR"),
    target_size=(224, 224),
    batch_size=32,
    subset='training',
    class_mode='categorical',
    shuffle=False
)

# üîπ CT generator
ct_train = img_gen.flow_from_directory(
    os.path.join(data_path, "CT"),
    target_size=(224, 224),
    batch_size=32,
    subset='training',
    class_mode='categorical',
    shuffle=False
)

# üîπ Audio generator
audio_train = img_gen.flow_from_directory(
    os.path.join(data_path, "Cough sound"),
    target_size=(128, 128),
    color_mode='grayscale',
    batch_size=32,
    subset='training',
    class_mode='categorical',
    shuffle=False
)

#  CT
ct_train = img_gen.flow_from_directory(
    os.path.join(data_path, "CT"),
    target_size=(224, 224),
    batch_size=64,
    subset='training',
    class_mode='categorical'
)

ct_val = img_gen.flow_from_directory(
    os.path.join(data_path, "CT"),
    target_size=(224, 224),
    batch_size=64,
    subset='validation',
    class_mode='categorical'
)

#  Cough sound
audio_train = img_gen.flow_from_directory(
    os.path.join(data_path, "Cough sound"),
    target_size=(128, 128),
    color_mode='grayscale',   #
    batch_size=64,
    subset='training',
    class_mode='categorical'
)

audio_val = img_gen.flow_from_directory(
    os.path.join(data_path, "Cough sound"),
    target_size=(128, 128),
    color_mode='grayscale',
    batch_size=64,
    subset='validation',
    class_mode='categorical'
)

"""Intermediate Fusion Model"""

epochs = 10

steps_per_epoch = min(len(cxr_train), len(ct_train), len(audio_train))
val_steps = min(len(cxr_val), len(ct_val), len(audio_val))

history_loss = []
history_acc = []

for epoch in range(epochs):
    print(f"\nEpoch {epoch+1}/{epochs}")
    epoch_loss = 0
    epoch_acc = 0
    for step in range(steps_per_epoch):
        cxr_batch, y_cxr = next(cxr_train)
        ct_batch, y_ct = next(ct_train)
        audio_batch, y_audio = next(audio_train)

        loss, acc = fusion_model.train_on_batch(
            [cxr_batch, ct_batch, audio_batch],
            y_cxr
        )
        epoch_loss += loss
        epoch_acc += acc

        if step % 20 == 0:
            print(f" Step {step}/{steps_per_epoch} - loss: {loss:.4f} - acc: {acc:.4f}")

    avg_epoch_loss = epoch_loss / steps_per_epoch
    avg_epoch_acc = epoch_acc / steps_per_epoch
    history_loss.append(avg_epoch_loss)
    history_acc.append(avg_epoch_acc)
    print(f"Epoch {epoch+1} average loss: {avg_epoch_loss:.4f} - average acc: {avg_epoch_acc:.4f}")

print("\n End Model Train \n")

# validation
y_true, y_pred = [], []

for step in range(val_steps):
    cxr_batch, y_val = next(cxr_val)
    ct_batch, _ = next(ct_val)
    audio_batch, _ = next(audio_val)

    preds = fusion_model.predict([cxr_batch, ct_batch, audio_batch])
    y_true.extend(np.argmax(y_val, axis=1))
    y_pred.extend(np.argmax(preds, axis=1))

print("\n Classification Report:\n")
print(classification_report(y_true, y_pred, target_names=cxr_train.class_indices.keys()))

import matplotlib.pyplot as plt

epochs_range = range(1, len(history_loss) + 1)

plt.figure(figsize=(10, 4))

# Loss
plt.subplot(1, 2, 1)
plt.plot(epochs_range, history_loss, marker='o', color='red')
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(epochs_range, history_acc, marker='o', color='green')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

classes = list(cxr_train.class_indices.keys())
cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(classes)))

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

for layer in fusion_model.layers:
    if 'conv' in layer.name:
        print(layer.name)

data_path_im = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3/processed_dataset/processed_dataset"
print("Contents of data_path_im:", os.listdir(data_path_im))

cxr_path = os.path.join(data_path_im, "CXR" , "covid")
print(os.listdir(cxr_path))

!pip install lime tf-keras-vis --quiet

# EXPLAINABILITY VISUALIZATION
import tensorflow as tf
import cv2
from lime import lime_image
from skimage.segmentation import mark_boundaries
from tf_keras_vis.saliency import Saliency
from tf_keras_vis.utils.model_modifiers import ReplaceToLinear
from tf_keras_vis.utils.scores import CategoricalScore
from tensorflow.keras.models import Model

def get_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    grad_model = Model([model.inputs],
                       [model.get_layer(last_conv_layer_name).output, model.output])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]
    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)

    heatmap = np.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def overlay_heatmap(heatmap, image, alpha=0.4, colormap=cv2.COLORMAP_JET):
    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))
    heatmap = np.uint8(255 * heatmap)
    heatmap = cv2.applyColorMap(heatmap, colormap)
    output = cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)
    return output

# Load test image
image_path = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3/processed_dataset/processed_dataset/CXR/covid/covid44.jpg"

img = tf.keras.preprocessing.image.load_img(image_path, target_size=(224,224))
img_array = tf.keras.preprocessing.image.img_to_array(img)
img_array = np.expand_dims(img_array/255.0, axis=0)

# Grad-CAM
heatmap = get_gradcam_heatmap(
    [img_array, np.zeros((1,224,224,3)), np.zeros((1,128,128,1))],
    fusion_model,
    last_conv_layer_name='conv2d_1'
)
gradcam_image = overlay_heatmap(heatmap, np.uint8(img_array[0]*255))

#  LIME
explainer = lime_image.LimeImageExplainer()
explanation = explainer.explain_instance(
    np.uint8(img_array[0]*255),
    classifier_fn=lambda x: fusion_model.predict(
        [x, np.zeros((x.shape[0],224,224,3)), np.zeros((x.shape[0],128,128,1))]
    ),
    top_labels=1,
    hide_color=0,
    num_samples=1000
)
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=8,
    hide_rest=False
)
lime_image_result = mark_boundaries(temp/255.0, mask)

#  Saliency Map
score = CategoricalScore(0)
saliency = Saliency(fusion_model, model_modifier=ReplaceToLinear(), clone=True)
saliency_map = saliency(score, [img_array, np.zeros((1,224,224,3)), np.zeros((1,128,128,1))])
saliency_map = saliency_map[0]
saliency_map = (saliency_map - saliency_map.min()) / (saliency_map.max() - saliency_map.min())

# Display results
fig, axes = plt.subplots(1, 3, figsize=(15,5))

axes[0].imshow(cv2.cvtColor(gradcam_image, cv2.COLOR_BGR2RGB))
axes[0].set_title("Grad-CAM")

axes[1].imshow(lime_image_result)
axes[1].set_title("LIME")

saliency_to_show = np.squeeze(saliency_map)
axes[2].imshow(saliency_to_show, cmap='hot')
axes[2].set_title("Saliency Map")

for ax in axes:
    ax.axis('off')

plt.tight_layout()
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, Lambda
from tensorflow.keras.models import Model


# Inputs
input_cxr = Input(shape=(224, 224, 3), name='CXR_Input')
input_ct = Input(shape=(224, 224, 3), name='CT_Input')
input_audio = Input(shape=(128, 128, 1), name='Audio_Input')

# Step 1: Extract low-level features (early stage)
# Lightweight Conv block for each modality
def early_features(x, name_prefix):
    x = Conv2D(16, (3,3), activation='relu', padding='same', name=f'{name_prefix}_conv1')(x)
    x = MaxPooling2D((2,2), name=f'{name_prefix}_pool1')(x)
    return x

cxr_feat = early_features(input_cxr, 'CXR')
ct_feat = early_features(input_ct, 'CT')
audio_feat = early_features(input_audio, 'Audio')

# Step 2: Resize audio feature map to match image dimensions using Lambda layer
audio_feat_resized = Lambda(lambda z: tf.image.resize(z, (cxr_feat.shape[1], cxr_feat.shape[2])))(audio_feat)


# Step 3: Early Fusion (concatenate features)
merged = Concatenate(axis=-1)([cxr_feat, ct_feat, audio_feat_resized])

# Step 4: Continue with shared CNN layers (common feature extractor)
x = Conv2D(64, (3,3), activation='relu', padding='same')(merged)
x = MaxPooling2D((2,2))(x)
x = Conv2D(128, (3,3), activation='relu', padding='same')(x)
x = MaxPooling2D((2,2))(x)
x = Flatten()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.4)(x)

# Step 5: Output
output = Dense(4, activation='softmax')(x)

# Model
early_fusion_model = Model(inputs=[input_cxr, input_ct, input_audio], outputs=output)

early_fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
early_fusion_model.summary()

import os

base_path = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3"

if "processed_dataset" in os.listdir(base_path):
    data_path = os.path.join(base_path, "processed_dataset")
else:
    data_path = base_path

print("‚úÖ Final data_path:", data_path)
print("üìÅ Folders inside:", os.listdir(data_path))

# ==========================================================
#                IMPORTS
# ==========================================================
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Concatenate, Lambda
from tensorflow.keras.models import Model
from sklearn.metrics import accuracy_score, classification_report


# ==========================================================
#                DATA PATH CHECK
# ==========================================================
base_path = "/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3/processed_dataset"

# Detect correct internal folder automatically
if "processed_dataset" in os.listdir(base_path):
    data_path = os.path.join(base_path, "processed_dataset")
else:
    data_path = base_path

print("‚úÖ Final data_path:", data_path)
print("üìÅ Folders inside:", os.listdir(data_path))


# ==========================================================
#                DATA GENERATORS
# ==========================================================
img_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

# CXR
cxr_train = img_gen.flow_from_directory(
    os.path.join(data_path, "CXR"),
    target_size=(224, 224),
    batch_size=64,
    subset='training',
    class_mode='categorical',
    shuffle=True
)

cxr_val = img_gen.flow_from_directory(
    os.path.join(data_path, "CXR"),
    target_size=(224, 224),
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

# CT
ct_val = img_gen.flow_from_directory(
    os.path.join(data_path, "CT"),
    target_size=(224, 224),
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

# Cough
audio_val = img_gen.flow_from_directory(
    os.path.join(data_path, "Cough sound"),
    target_size=(128, 128),
    color_mode='grayscale',
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

val_steps = min(len(cxr_val), len(ct_val), len(audio_val))


# ==========================================================
#                INTERMEDIATE FUSION MODEL
# ==========================================================
input_cxr = Input(shape=(224, 224, 3), name='CXR_Input')
x1 = Conv2D(32, (3,3), activation='relu', padding='same')(input_cxr)
x1 = MaxPooling2D((2,2))(x1)
x1 = Conv2D(64, (3,3), activation='relu', padding='same')(x1)
x1 = MaxPooling2D((2,2))(x1)
x1 = Flatten()(x1)
x1 = Dense(128, activation='relu')(x1)

input_ct = Input(shape=(224, 224, 3), name='CT_Input')
x2 = Conv2D(32, (3,3), activation='relu', padding='same')(input_ct)
x2 = MaxPooling2D((2,2))(x2)
x2 = Conv2D(64, (3,3), activation='relu', padding='same')(x2)
x2 = MaxPooling2D((2,2))(x2)
x2 = Flatten()(x2)
x2 = Dense(128, activation='relu')(x2)

input_audio = Input(shape=(128, 128, 1), name='Audio_Input')
x3 = Conv2D(32, (3,3), activation='relu', padding='same')(input_audio)
x3 = MaxPooling2D((2,2))(x3)
x3 = Conv2D(64, (3,3), activation='relu', padding='same')(x3)
x3 = MaxPooling2D((2,2))(x3)
x3 = Flatten()(x3)
x3 = Dense(128, activation='relu')(x3)

merged = Concatenate()([x1, x2, x3])
z = Dense(256, activation='relu')(merged)
z = Dropout(0.4)(z)
output = Dense(4, activation='softmax', name='Output')(z)

fusion_model = Model(inputs=[input_cxr, input_ct, input_audio], outputs=output)
fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
fusion_model.summary()


# ==========================================================
#                EARLY FUSION MODEL
# ==========================================================
input_cxr_early = Input(shape=(224, 224, 3), name='CXR_Input_early')
input_ct_early = Input(shape=(224, 224, 3), name='CT_Input_early')
input_audio_early = Input(shape=(128, 128, 1), name='Audio_Input_early')

def early_features(x, name_prefix):
    x = Conv2D(16, (3,3), activation='relu', padding='same', name=f'{name_prefix}_conv1')(x)
    x = MaxPooling2D((2,2), name=f'{name_prefix}_pool1')(x)
    return x

cxr_feat_early = early_features(input_cxr_early, 'CXR_early')
ct_feat_early = early_features(input_ct_early, 'CT_early')
audio_feat_early = early_features(input_audio_early, 'Audio_early')

audio_feat_resized_early = Lambda(lambda z: tf.image.resize(z, (cxr_feat_early.shape[1], cxr_feat_early.shape[2])))(audio_feat_early)

merged_early = Concatenate(axis=-1)([cxr_feat_early, ct_feat_early, audio_feat_resized_early])

x_early = Conv2D(64, (3,3), activation='relu', padding='same')(merged_early)
x_early = MaxPooling2D((2,2))(x_early)
x_early = Conv2D(128, (3,3), activation='relu', padding='same')(x_early)
x_early = MaxPooling2D((2,2))(x_early)
x_early = Flatten()(x_early)
x_early = Dense(256, activation='relu')(x_early)
x_early = Dropout(0.4)(x_early)
output_early = Dense(4, activation='softmax')(x_early)

early_fusion_model = Model(inputs=[input_cxr_early, input_ct_early, input_audio_early], outputs=output_early)
early_fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
early_fusion_model.summary()


# ==========================================================
#                TRAIN BOTH MODELS
# ==========================================================
epochs = 5
steps_per_epoch = min(len(cxr_train), len(ct_val), len(audio_val))

# Train the early fusion model
epochs_early = 10 # You can adjust the number of epochs
steps_per_epoch_early = min(len(cxr_train), len(ct_train), len(audio_train)) # Use training generators

history_loss_early = []
history_acc_early = []

print("\nStarting Early Fusion Model Training...\n")

for epoch in range(epochs_early):
    print(f"\nEpoch {epoch+1}/{epochs_early}")
    epoch_loss_early = 0
    epoch_acc_early = 0
    for step in range(steps_per_epoch_early):
        cxr_batch, y_cxr = next(cxr_train)
        ct_batch, y_ct = next(ct_train) # Assuming ct_train exists, otherwise use ct_val if only validation generators are available for CT
        audio_batch, y_audio = next(audio_train) # Assuming audio_train exists, otherwise use audio_val

        loss, acc = early_fusion_model.train_on_batch(
            [cxr_batch, ct_batch, audio_batch],
            y_cxr # Assuming labels are consistent across modalities
        )
        epoch_loss_early += loss
        epoch_acc_early += acc

        if step % 20 == 0:
            print(f" Step {step}/{steps_per_epoch_early} - loss: {loss:.4f} - acc: {acc:.4f}")

    avg_epoch_loss_early = epoch_loss_early / steps_per_epoch_early
    avg_epoch_acc_early = epoch_acc_early / steps_per_epoch_early
    history_loss_early.append(avg_epoch_loss_early)
    history_acc_early.append(avg_epoch_acc_early)
    print(f"Epoch {epoch+1} average loss: {avg_epoch_loss_early:.4f} - average acc: {avg_epoch_acc_early:.4f}")

print("\nEarly Fusion Model Training Finished.\n")

# Evaluate Intermediate Fusion Model
y_true_inter, y_pred_inter = [], []

val_steps = min(len(cxr_val), len(ct_val), len(audio_val))

for step in range(val_steps):
    cxr_batch, y_val = next(cxr_val)
    ct_batch, _ = next(ct_val)
    audio_batch, _ = next(audio_val)

    preds = fusion_model.predict([cxr_batch, ct_batch, audio_batch], verbose=0)
    y_true_inter.extend(np.argmax(y_val, axis=1))
    y_pred_inter.extend(np.argmax(preds, axis=1))

print("Intermediate Fusion Classification Report:")
print(classification_report(y_true_inter, y_pred_inter, target_names=cxr_val.class_indices.keys()))

# Evaluate Early Fusion Model
y_true_early, y_pred_early = [], []

val_steps = min(len(cxr_val), len(ct_val), len(audio_val)) # Ensure using validation generators

for step in range(val_steps):
    cxr_batch, y_val = next(cxr_val)
    ct_batch, _ = next(ct_val)
    audio_batch, _ = next(audio_val)

    preds = early_fusion_model.predict([cxr_batch, ct_batch, audio_batch], verbose=0)
    y_true_early.extend(np.argmax(y_val, axis=1))
    y_pred_early.extend(np.argmax(preds, axis=1))

print("Early Fusion Classification Report:")
print(classification_report(y_true_early, y_pred_early, target_names=cxr_val.class_indices.keys()))

# Compare Accuracies Visually
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

acc_inter = accuracy_score(y_true_inter, y_pred_inter)
acc_early = accuracy_score(y_true_early, y_pred_early)

models = ['Intermediate Fusion', 'Early Fusion']
accuracies = [acc_inter * 100, acc_early * 100]

plt.figure(figsize=(6, 4))
bars = plt.bar(models, accuracies, color=['blue', 'orange']) # Use different colors

plt.title('Model Accuracy Comparison', fontsize=14)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.ylim(0, 100)

for i, v in enumerate(accuracies):
    plt.text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')

plt.show()

cxr_train = img_gen.flow_from_directory(
    os.path.join("/root/.cache/kagglehub/datasets/hossamfakher/cxr-ct-cough/versions/3/processed_dataset/processed_dataset", "CXR"),
    target_size=(224, 224),
    batch_size=64,
    subset='training',
    class_mode='categorical'
)

# ---- Recreate validation generators for Early Fusion ----
img_gen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

cxr_val = img_gen.flow_from_directory(
    os.path.join(data_path, "CXR"),
    target_size=(224, 224),
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

ct_val = img_gen.flow_from_directory(
    os.path.join(data_path, "CT"),
    target_size=(224, 224),
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

audio_val = img_gen.flow_from_directory(
    os.path.join(data_path, "Cough sound"),
    target_size=(128, 128),
    color_mode='grayscale',
    batch_size=64,
    subset='validation',
    class_mode='categorical',
    shuffle=False
)

# ---- Now evaluate Early Fusion ----
y_true_early, y_pred_early = [], []

val_steps = min(len(cxr_val), len(ct_val), len(audio_val))

for step in range(val_steps):
    cxr_batch, y_val = next(cxr_val)
    ct_batch, _ = next(ct_val)
    audio_batch, _ = next(audio_val)

    preds = early_fusion_model.predict([cxr_batch, ct_batch, audio_batch], verbose=0)
    y_true_early.extend(np.argmax(y_val, axis=1))
    y_pred_early.extend(np.argmax(preds, axis=1))

if len(y_true_early) == 0 or len(y_pred_early) == 0:
    print("‚ö†Ô∏è No predictions generated for Early Fusion. Check val_steps or reset generators.")

# ==========================================================
#                VISUAL COMPARISON (BAR CHART)
# ==========================================================
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

# Recalculate accuracies just to be safe
acc_inter = accuracy_score(y_true_inter, y_pred_inter)
print(f"\nIntermediate Fusion Accuracy: {acc_inter*100:.2f}%")

acc_early = accuracy_score(y_true_early, y_pred_early)
print(f"Early Fusion Accuracy: {acc_early*100:.2f}%")

# Model names and accuracies
models = ['Intermediate Fusion', 'Early Fusion']
accuracies = [acc_inter * 100, acc_early * 100]

# Create bar chart
plt.figure(figsize=(6,4))
bars = plt.bar(models, accuracies)

# Highlight the better model
if acc_early > acc_inter:
    bars[1].set_color('green')
    bars[0].set_color('gray')
else:
    bars[0].set_color('green')
    bars[1].set_color('gray')

# Chart styling
plt.title('Model Accuracy Comparison', fontsize=14)
plt.ylabel('Accuracy (%)', fontsize=12)
plt.ylim(0, 100)

# Display accuracy values above bars
for i, v in enumerate(accuracies):
    plt.text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')

plt.show()

# ==========================================================
#           üìä COMPARE CXR AND CT CLASSIFICATION MODELS
# ==========================================================
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# ===============================
# üîπ Evaluate both models
# ===============================
cxr_val_loss, cxr_val_acc = cxr_model.evaluate(cxr_val, verbose=0)
ct_val_loss, ct_val_acc = ct_model.evaluate(ct_val, verbose=0)

# Convert accuracy to percentage
cxr_val_acc *= 100
ct_val_acc *= 100

# ===============================
# üîπ Create DataFrame for comparison
# ===============================
comparison_df = pd.DataFrame({
    'Model': ['CXR Classification', 'CT Classification'],
    'Accuracy (%)': [cxr_val_acc, ct_val_acc],
    'Loss': [cxr_val_loss, ct_val_loss]
})

print("\nüìà Model Comparison:\n")
print(comparison_df)

# ===============================
# üé® Accuracy Comparison Bar Chart
# ===============================
plt.figure(figsize=(6,4))
bars = plt.bar(comparison_df['Model'], comparison_df['Accuracy (%)'], color=['skyblue', 'lightcoral'])

# Highlight best model in green
best_idx = comparison_df['Accuracy (%)'].idxmax()
bars[best_idx].set_color('green')

# Add accuracy labels
for i, v in enumerate(comparison_df['Accuracy (%)']):
    plt.text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')

plt.title('CXR vs CT Model Accuracy', fontsize=14)
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
plt.show()

# ===============================
# üé® Loss Comparison Bar Chart
# ===============================
plt.figure(figsize=(6,4))
bars = plt.bar(comparison_df['Model'], comparison_df['Loss'], color=['skyblue', 'lightcoral'])

# Highlight lowest loss in green
best_idx = comparison_df['Loss'].idxmin()
bars[best_idx].set_color('green')

# Add loss labels
for i, v in enumerate(comparison_df['Loss']):
    plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')

plt.title('CXR vs CT Model Loss', fontsize=14)
plt.ylabel('Loss')
plt.show()

# Define individual CXR model
input_cxr_single = Input(shape=(224, 224, 3), name='CXR_Input_Single')
x_cxr = Conv2D(32, (3,3), activation='relu', padding='same')(input_cxr_single)
x_cxr = MaxPooling2D((2,2))(x_cxr)
x_cxr = Conv2D(64, (3,3), activation='relu', padding='same')(x_cxr)
x_cxr = MaxPooling2D((2,2))(x_cxr)
x_cxr = Flatten()(x_cxr)
x_cxr = Dense(128, activation='relu')(x_cxr)
x_cxr = Dropout(0.4)(x_cxr)
output_cxr = Dense(4, activation='softmax', name='CXR_Output')(x_cxr)

cxr_model = Model(inputs=input_cxr_single, outputs=output_cxr)

# Define individual CT model
input_ct_single = Input(shape=(224, 224, 3), name='CT_Input_Single')
x_ct = Conv2D(32, (3,3), activation='relu', padding='same')(input_ct_single)
x_ct = MaxPooling2D((2,2))(x_ct)
x_ct = Conv2D(64, (3,3), activation='relu', padding='same')(x_ct)
x_ct = MaxPooling2D((2,2))(x_ct)
x_ct = Flatten()(x_ct)
x_ct = Dense(128, activation='relu')(x_ct)
x_ct = Dropout(0.4)(x_ct)
output_ct = Dense(4, activation='softmax', name='CT_Output')(x_ct)

ct_model = Model(inputs=input_ct_single, outputs=output_ct)

# ==========================================================
#           üìä COMPARE ALL FOUR MODELS (CXR, CT, Inter, Early)
# ==========================================================

# ==========================================================
#       1Ô∏è‚É£ Evaluate CXR & CT Models (Direct Evaluation)
# ==========================================================
cxr_val_loss, cxr_val_acc = cxr_model.evaluate(cxr_val, verbose=0)
ct_val_loss, ct_val_acc = ct_model.evaluate(ct_val, verbose=0)

# ==========================================================
#       2Ô∏è‚É£ Evaluate Fusion Models (Using Predictions)
# ==========================================================
acc_inter = accuracy_score(y_true_inter, y_pred_inter)
acc_early = accuracy_score(y_true_early, y_pred_early)

# Optional: add losses for fusion models if available
# inter_loss = None  # Replace with actual validation loss if you have it
# early_loss = None  # Replace with actual validation loss if you have it

# ==========================================================
#       3Ô∏è‚É£ Create a DataFrame for All Models
# ==========================================================
comparison_df = pd.DataFrame({
    'Model': ['CXR Classification', 'CT Classification', 'Intermediate Fusion', 'Early Fusion'],
    'Accuracy (%)': [
        cxr_val_acc * 100,
        ct_val_acc * 100,
        acc_inter * 100,
        acc_early * 100
    ],
    'Loss': [
        cxr_val_loss,
        ct_val_loss,
        None, # Assuming loss is not directly available from prediction lists
        None  # Assuming loss is not directly available from prediction lists
    ]
})

print("\nüìä Overall Model Comparison:\n")
print(comparison_df)

# ==========================================================
#       4Ô∏è‚É£ Accuracy Comparison Bar Chart
# ==========================================================
plt.figure(figsize=(8,5))
bars = plt.bar(comparison_df['Model'], comparison_df['Accuracy (%)'], color=['steelblue', 'salmon', 'gray', 'orange'])

# Highlight the best-performing model in green
best_idx = comparison_df['Accuracy (%)'].idxmax()
bars[best_idx].set_color('green')

# Add accuracy values above bars
for i, v in enumerate(comparison_df['Accuracy (%)']):
    plt.text(i, v + 1, f'{v:.2f}%', ha='center', fontweight='bold')

plt.title('Overall Model Accuracy Comparison', fontsize=14) # Updated title
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.show()

# ==========================================================
#       5Ô∏è‚É£ Loss Comparison Bar Chart (if losses are available)
# ==========================================================
# Check if loss data is available before plotting
if not comparison_df['Loss'].isnull().all():
    plt.figure(figsize=(8,5))
    bars_loss = plt.bar(comparison_df['Model'], comparison_df['Loss'], color=['steelblue', 'salmon', 'gray', 'orange'])

    # Highlight lowest loss in green (excluding None values)
    valid_losses = comparison_df.dropna(subset=['Loss'])
    if not valid_losses.empty:
        best_loss_idx = valid_losses['Loss'].idxmin()
        bars_loss[best_loss_idx].set_color('green')

    # Add loss labels (only for non-None values)
    for i, v in enumerate(comparison_df['Loss']):
        if pd.notna(v):
             plt.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')

    plt.title('Overall Model Loss Comparison', fontsize=14) # Updated title
    plt.ylabel('Loss')
    plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
    plt.tight_layout() # Adjust layout to prevent labels overlapping
    plt.show()
else:
    print("\nLoss comparison chart not generated: Loss data not available for all models.")